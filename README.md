# Computationally-Efficient-Text-to-Video-Editing-for-Autonomous-Driving-Data-Augmentation
IEEE IV 2026

![overview](https://github.com/user-attachments/assets/b23af282-f97b-4e89-89b2-41fa14210e31)


This repository contains qualitative results of the method "Computationally Efficient Text-to-Video Editing for
Autonomous Driving Data Augmentation" (accepted at IEEE IV 2026) by Thérèse Tisseau des Escotais, Adam Diakite, Bertrand Leroy, Javier Ibanez-Guzman, Clément Rambour, and Arnaud Breloy.

## Edited videos
Here are some examples of style editing on driving videos from [**BDD100K**](https://github.com/bdd100k/bdd100k).
For each video, you can watch the input video, results from two baselines ([**VidToMe**](https://github.com/lixirui142/VidToMe) and [**RAVE**](https://github.com/rehg-lab/RAVE)), and our results.
Detected objects are aggregated into three classes: "vehicle" in red, "VRU" in green and "traffic sign" in blue.

