# Computationally-Efficient-Text-to-Video-Editing-for-Autonomous-Driving-Data-Augmentation
IEEE IV 2026

![overview](https://github.com/user-attachments/assets/b23af282-f97b-4e89-89b2-41fa14210e31)


This repository contains qualitative results of the method "Computationally Efficient Text-to-Video Editing for
Autonomous Driving Data Augmentation" (accepted at IEEE IV 2026) by Thérèse Tisseau des Escotais, Adam Diakite, Bertrand Leroy, Javier Ibanez-Guzman, Clément Rambour, and Arnaud Breloy.

## Edited videos
Here are some examples of style editing on driving videos from [**BDD100K**](https://github.com/bdd100k/bdd100k).
For each video, you can watch the input video, results from two baselines ([**VidToMe**](https://github.com/lixirui142/VidToMe) and [**RAVE**](https://github.com/rehg-lab/RAVE)), and our results.
Detected objects are aggregated into three classes: "vehicle" in red, "VRU" in green and "traffic sign" in blue.


<video src="https://github.com/user-attachments/assets/8bfc2ecb-1968-4675-abe6-2fc84702eb73"
       autoplay
       loop
       muted
       playsinline
       width="800">
</video>

https://github.com/user-attachments/assets/8bfc2ecb-1968-4675-abe6-2fc84702eb73


https://github.com/user-attachments/assets/3e9edf36-be44-4215-95b8-33da63a56d8b



https://github.com/user-attachments/assets/6f167893-7a00-41a3-8caa-18a7302c93d5



https://github.com/user-attachments/assets/2124c200-003a-400d-8cac-8ba53867a124


